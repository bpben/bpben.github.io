---
layout: post
title: The Embiggening of NLP - Part 2 - LARGE Language Models
---

Continuing where we [left off last week](https://bpben.github.io/2023/07/20/intro_llm/), let's get into some BIG topics.  We're going to be large and in charge.  This will be no small task.

And so on...

<h3> The Embiggening </h3>

So what is large?

Well, it's kind of unclear.  Large mainly refers to the number of parameters in a model.  Parameters, simply, are the values learned by the model through viewing data.  Data is the other "large" part of LLMs.  GPT-3 is 175 billion parameters, trained on [753 billion GB](https://s10251.pcdn.co/pdf/2022-Alan-D-Thompson-Whats-in-my-AI-Rev-0.pdf) worth of text.  GPT-4 is [rumored to have 1.8 trillion parameters](https://medium.com/@daniellefranca96/gpt4-all-details-leaked-48fa20f9a4a).  Compare that to BERT, which has 345 million parameters.  So we're talking LARGE.

But _why_ is large?

There's a concept that's been around since [around 2008](https://citeseerx.ist.psu.edu/document?doi=ee0a332b4fc1e82a9999acd6cebceb165dc8645b) called "zero-shot" learning.  Essentially, it means that a model can give an answer to a question without ever having been explicitly trained to do so.  The simple example is that you can ask even a "small" LLM what the capital of France is, and it'll say "Paris".

How is this possible? 

Well, think back what I described about word2vec in the [last post](https://bpben.github.io/2023/07/20/intro_llm/), these models create "representations" of each word.  If you decompose these representations in two dimensional space:

| ![Word2vec illustration of countries and their capitols]({{site.url}}/assets/llms/countries_capitals.png){:height="30%" width="30%"} |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| 
| Word2vec illustration of countries and their capitols [source](https://wiki.pathmind.com/images/wiki/countries_capitals.png)                                 |

You'll notice that all the capitols are *roughly* similar distance from their countries.  You might even be able to extract a country-capitol vector that would translate a country into its capital.  (In real life, it's [not so simple](https://www.youtube.com/watch?v=bEWQ1ckRQ3w)).  

Remember, this all comes from the model reading enough text.  It's not explicitly trained to represent countries and their capitals in this way.  One word to describe this is an "emergent" property.

LLMs go one level deeper in that they appear to be able to "understand" the relationship between countries and capitals (think of that "country-capital" vector).  I use "understand" in quotes because it's still a bit of a mystery what these models actually "understand".  But it is the case that, without explicitly being trained to answer the question, the LLM provides the correct response to "What is the capital of France?".

A note here - I'm calling this zero-shot learning, but it may be better called "in-context learning".  The main difference is that zero shot learning may actually involve some amount of model training.  For me, that's the main difference.  The input prompts may be the same, but in-context learning does not change the model.

Prompts, you say? Why, isn't that the radical new job everyone's talking about?

Right you are - LLMs can do much more than tell you about France.  You just have to: 

<h3> Say the magic word </h3>
One fascinating thing about these LLMs is that their behavior can be drastically changed based on the input.  This is related to the zero-shot/in-context learning example above.  The model's representation of each word of the prompt is contingent on the other words.  So if you change the context, even a little bit, you'll change the representation across the sequence.

What does this look like? I thought this is an interesting example with ChatGPT.  Here I provide a conversation, without context.  One person is speaking English and I want B to give responses as if they were a French Yoda because I'm the internet and I want what I want.

![LLM assistant failing to answer correctly]({{site.url}}/assets/llms/translate.png){:height="15%" width="15%"}

ChatGPT is impressive in that without any additional information, it understands that B is repeating A, but in French.  However, it doesn't feel like talking like French Yoda.  That is, unless I ask nicely:

![LLM assistant failing to answer correctly]({{site.url}}/assets/llms/yoda_translate.png){:height="40%" width="40%"}

Congrats, ya'll are now prompt engineers.  Now let it do your math homework for you:

| ![LLM assistant failing to answer correctly]({{site.url}}/assets/llms/fail_math.png){:height="30%" width="30%"}              |
|------------------------------------------------------------------------------------------------------------------------------| 
| LLM assistant failing to answer correctly [source](https://arxiv.org/pdf/2205.11916.pdf) 

Uh-oh.  Now math is [notoriously difficult](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision#fn-2) for LLMs.  But this is a pretty simple logic puzzle.  Maybe there's some way to break down the problem?

<h3> Now show your work </h3>

Another pretty incredible "emergent property" of LLMs is that they can show their work.  If you've used ChatGPT, you'll see it's pretty chatty and impressed with itself:

![ChatGPT being chatty...gpt]({{site.url}}/assets/llms/chatty.png){:height="30%" width="30%"}                                

I asked it to be succinct here, but if you ask it this question it will automatically describe its "thought" process.  An interesting finding detailed in [Kojima et. al (2023)](https://arxiv.org/pdf/2205.11916.pdf) is that just by asking the model to work through its answer step by step, it can do better on some logic puzzles.  Theirs is the example above, here's the result when they ask the model to think step by step:

| ![LLM assistant answering correctly with Chain of Thought prompting]({{site.url}}/assets/llms/win_math.png){:height="30%" width="30%"} |
|----------------------------------------------------------------------------------------------------------------------------------------| 
| LLM assistant answering correctly with Chain of Thought prompting [source](https://arxiv.org/pdf/2205.11916.pdf)                       

You can see with the few-shot framing, the model is given an example of how to respond and it answers the question correctly.  When just ASKED to think step by step, it also responds correctly.

This is called Chain-of-Thought (CoT) prompting and is an area of active investigation.  It's another emergent property, particularly in _large_ LLMs, usually more than [100 billion parameters](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html).   

It's not clear [why this works](https://arxiv.org/abs/2212.10001) and, in some cases, either [the explanation or the answer or _both_ are wrong](https://arxiv.org/abs/2305.04388).

The term "emergent property", in my opinion, kind of obfuscates the fact that we're still trying to figure out exactly how this happens.  In some ways, it is kind of "magic".  Autocomplete somehow turns into useful assistants, it just needs to be big enough and asked politely enough.

Or does it? Something I feel is missing from a lot of the simplifications in the media is that ChatGPT is not _just_ autocomplete.  It's fit to purpose as an AI assistant.  And you don't become helpful by reading enough Wikipedia.  Just look at any Wikipedia article discussion, if you don't believe me.

In the next installment, I'll talk about how, like any instrument, these models need a lot of tuning to be useful.

So tune in.  Fine-tune, that is.

Sorry, will come up with a better pun by the time the next post is out.
